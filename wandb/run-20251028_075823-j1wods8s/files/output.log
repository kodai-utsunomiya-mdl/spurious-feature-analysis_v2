wandb is enabled and initialized.
Results will be saved to: results/wb_vit_scc_gf_example_2025-10-28_07-58-24
Using device: cuda

--- 1. Preparing Dataset ---

Displaying and saving 10 sample images from the WaterBirds train set...
Sample images saved to results/wb_vit_scc_gf_example_2025-10-28_07-58-24/dataset_samples.png

--- SCC-GF (Debias Method) Enabled ---
  [Warning] 'batch_size' config is ignored. Using full-batch (per-group) gradient calculation.
  Calculated statistics and weights:
  E[Y] (y_bar) = -0.5358
  E[A] (a_bar) = -0.4824
  w_g(-1.0, -1.0) = 0.569146
  w_g(-1.0, 1.0) = 0.198737
  w_g(1.0, -1.0) = 0.172042
  w_g(1.0, 1.0) = 0.060074

--- Train Set Group Distribution (WaterBirds) ---
Waterbird on Water (y=+1, a=+1)    :  1057 samples
Waterbird on Land (y=+1, a=-1)     :    56 samples
Landbird on Water (y=-1, a=+1)     :   184 samples
Landbird on Land (y=-1, a=-1)      :  3498 samples
Total                              :  4795 samples


--- Test Set Group Distribution (WaterBirds) ---
Waterbird on Water (y=+1, a=+1)    :   642 samples
Waterbird on Land (y=+1, a=-1)     :   642 samples
Landbird on Water (y=-1, a=+1)     :  2255 samples
Landbird on Land (y=-1, a=-1)      :  2255 samples
Total                              :  5794 samples


--- 2. Setting up Model and Optimizer ---
Downloading: "https://download.pytorch.org/models/vit_b_16-c867db91.pth" to /home/utsu/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth
100%|█████████████████████████████████████████████████████████████████████████████████| 330M/330M [00:02<00:00, 116MB/s]

Applying parametrization for 'ViT'...
        - WARNING: 'mf' scaling is not implemented for ViT.
        - Applying single Base LR (η = 1.00e-01) to all parameters.
        - All ViT parameters: LR = 1.00e-01

--- 3. Starting Training & Evaluation Loop ---
Traceback (most recent call last):
  File "/home/utsu/sp_v2/main.py", line 749, in <module>
    main(config_path='config.yaml')
  File "/home/utsu/sp_v2/main.py", line 598, in main
    scores_g, _ = model(X_g)
                  ^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1881, in _call_impl
    return inner()
           ^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1829, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/model.py", line 329, in forward
    output_scalar = self.vit(x)
                    ^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1488, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/functional.py", line 6307, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/utsu/sp_v2/.venv/lib/python3.12/site-packages/torch/nn/functional.py", line 5699, in _in_projection_packed
    proj = linear(q, w, b)
           ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.92 GiB. GPU 0 has a total capacity of 15.77 GiB of which 5.18 GiB is free. Process 3879934 has 10.59 GiB memory in use. Of the allocated memory 10.17 GiB is allocated by PyTorch, and 55.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
